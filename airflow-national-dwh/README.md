# ðŸ“Š National Statistics Data Warehouse (ETL Pipeline)

An end-to-end Data Warehouse project that demonstrates how to build and orchestrate
an ETL pipeline using Pentaho Data Integration (PDI) and Apache Airflow.

The project transforms raw national statistics data into structured fact tables
inside a PostgreSQL Data Warehouse.

## ðŸ—ï¸ Architecture
- Orchestration: Apache Airflow (Docker / Astro CLI)
- ETL Engine: Pentaho Data Integration (Kitchen.sh)
- Database: PostgreSQL
- Environment: Linux / Docker

## âš™ï¸ Technologies
- PostgreSQL
- Pentaho Data Integration (PDI)
- Apache Airflow
- Docker

## ðŸ§  Design Overview
- Raw data is ingested into landing tables
- Data is cleaned and standardized in staging tables
- Fact tables are loaded using Pentaho transformations
- A single Pentaho job orchestrates all fact loads
- Airflow triggers the Pentaho job and manages execution

## â–¶ï¸ How to Run (High Level)
1. Start Airflow using Astro CLI
2. Open Airflow UI
3. Trigger the DAG: national_statistics_dwh
4. Verify data in DWH fact tables

> Note: Environment-specific configurations (database host, credentials)
> are intentionally excluded from this public documentation.

## ðŸ“‚ Project Structure
national-statistics-dwh/
â”œâ”€â”€ dags/
â”œâ”€â”€ include/pentaho/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ packages.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ðŸŽ¯ Purpose
This project is built for learning, portfolio demonstration,
and showcasing real-world Data Engineering practices.

Created by: Abd El Rahman Sabri

